{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":5.733813,"end_time":"2023-11-16T17:36:14.129927","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-16T17:36:08.396114","version":"2.4.0"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"5b69fa0ed7564a94a5f4f7265add2e12":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8b64d3604bc94c19b842c81dad076081","IPY_MODEL_731559d0773043afb8b3982ea7d7b2eb","IPY_MODEL_0d34b1c2e89e41c0976335b5ba0e2801"],"layout":"IPY_MODEL_507f04527a684bcaae2f4566690ed2ce"}},"8b64d3604bc94c19b842c81dad076081":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_067ddfa71b6b4db294e7da9401478366","placeholder":"​","style":"IPY_MODEL_f3cee4884f4840a882f183f2219a7c81","value":"Training Fold 1 - Epoch 1:   0%"}},"731559d0773043afb8b3982ea7d7b2eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_02ea30660cb14a6396bdfb75b75e3652","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0e5017ec86648f9b9b6455d81f6e567","value":0}},"0d34b1c2e89e41c0976335b5ba0e2801":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a42882f0d6e14ff39627f470ba9d10ea","placeholder":"​","style":"IPY_MODEL_31cc7bd07d6f49b2be0463272fe1ebca","value":" 0/1 [00:00&lt;?, ?it/s]"}},"507f04527a684bcaae2f4566690ed2ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"067ddfa71b6b4db294e7da9401478366":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3cee4884f4840a882f183f2219a7c81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02ea30660cb14a6396bdfb75b75e3652":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0e5017ec86648f9b9b6455d81f6e567":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a42882f0d6e14ff39627f470ba9d10ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31cc7bd07d6f49b2be0463272fe1ebca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Introduction to Element Extraction using Multi-Task Learning with Adapter\n\nIn the realm of Natural Language Processing (NLP), understanding and extracting meaningful elements from textual data is paramount. This notebook explores the complexities of Element Extraction (EE), an essential process that identifies and classifies entities, aspects, and constraints within texts. We adopt a Multi-Task Learning (MTL) framework, utilizing the BIO tagging scheme for systematic annotation and extraction of these elements.\n\nOur approach integrates the advanced capabilities of RoBERTa with the strategic addition of a pre-trained \"AdapterHub/roberta-base-pf-qnli\" adapter. This adapter, specifically tailored for question-answering tasks, enhances RoBERTa's ability to discern nuanced information within questions, making it particularly effective for the EE task in comparative analysis contexts.\n\nThe MTL framework employs a robust architecture that not only leverages RoBERTa's transformer-based model but also fine-tunes it with the QNLI adapter to improve task-specific performance. Through a series of Python code snippets, we will detail the implementation of this enhanced model, providing insights and explanations at each step. This exploration aims to illuminate the technical underpinnings of Element Extraction while showcasing the practical applications and benefits of integrating specialized adapters in NLP tasks.\n","metadata":{}},{"cell_type":"markdown","source":"## Data Acquisition for Element Extraction\n\nBefore diving into the intricacies of our Element Extraction model, the first crucial step is to acquire the relevant dataset. In the following code snippet, we utilize the `requests` library to download a pre-compiled dataset from a specified URL. Upon successful download, we proceed to unzip the dataset using the `zipfile` library, preparing our data for the upcoming processing stages. This initial setup ensures we have the necessary data in an accessible format, paving the way for the element extraction tasks.","metadata":{}},{"cell_type":"code","source":"import requests\nimport zipfile\nimport os\n\ndata_url = \"https://github.com/mahsamb/SCRQD/raw/main/Dataset.zip\"\nzip_filename = \"Dataset.zip\"\n\n# Downloading using requests\nresponse = requests.get(data_url)\n\n# Check if the request was successful (status_code 200)\nif response.status_code == 200:\n    with open(zip_filename, \"wb\") as f:\n        f.write(response.content)\nelse:\n    print(f\"Failed to retrieve the data: {response.status_code}: {response.text}\")\n    # Add additional error handling here\n\n# Unzipping the dataset\ntry:\n    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n        zip_ref.extractall(\"data\")\n    print(\"Files extracted:\")\n    print(os.listdir(\"data\"))\nexcept zipfile.BadZipFile:\n    print(\"Error: The file doesn’t appear to be a valid zip file\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:32:25.199661Z","iopub.execute_input":"2024-03-29T08:32:25.200441Z","iopub.status.idle":"2024-03-29T08:32:25.296996Z","shell.execute_reply.started":"2024-03-29T08:32:25.200412Z","shell.execute_reply":"2024-03-29T08:32:25.296106Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Files extracted:\n['Relations.pkl', 'EntityRoles.pkl', 'ComparativePreferences.pkl', 'Elements.pkl', 'Questions.pkl']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preparing the Environment and Loading Data\n\nTo ensure our Element Extraction model functions effectively, we start by setting up our environment with essential libraries such as `numpy` and `pandas` for data manipulation, and `re` for regular expressions, which are critical for processing text data. Additionally, we use `pickle` for loading our","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pickle as cPickle\nimport pickle\nimport re\nimport pandas as pd\nfrom IPython.display import display, HTML\nimport random\n\n\nwith open(r\"/kaggle/working/data/Questions.pkl\", \"rb\") as input_file:\n    QuestionDict = pickle.load(input_file)\n    input_file.close()\n\nwith open(r\"/kaggle/working/data/Elements.pkl\", \"rb\") as input_file:\n    Product_Aspect_Contraint_dict = pickle.load(input_file)\n    input_file.close()","metadata":{"papermill":{"duration":0.526316,"end_time":"2023-11-16T17:36:12.801063","exception":false,"start_time":"2023-11-16T17:36:12.274747","status":"completed"},"tags":[],"id":"fJSN_w3-GKjl","execution":{"iopub.status.busy":"2024-03-29T08:32:25.298334Z","iopub.execute_input":"2024-03-29T08:32:25.298601Z","iopub.status.idle":"2024-03-29T08:32:26.436334Z","shell.execute_reply.started":"2024-03-29T08:32:25.298578Z","shell.execute_reply":"2024-03-29T08:32:26.435377Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"max1=100","metadata":{"id":"rU2hhB6I3VxZ","execution":{"iopub.status.busy":"2024-03-29T08:32:26.437441Z","iopub.execute_input":"2024-03-29T08:32:26.437830Z","iopub.status.idle":"2024-03-29T08:32:26.442181Z","shell.execute_reply.started":"2024-03-29T08:32:26.437805Z","shell.execute_reply":"2024-03-29T08:32:26.441264Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Initial Exploration of Loaded Datasets\n\nWith our datasets loaded into Python dictionaries, it's essential to begin with an initial exploration to understand the structure and type of data we'll be working with. In the code snippets provided, we iterate through both `Product_Aspect_Contraint_dict` and `QuestionDict`, printing the first key-value pair of each to get a glimpse into the data. This preliminary step is crucial for ensuring the integrity of our data and to familiarize ourselves with the dataset's format, which will inform our strategy for the Element Extraction process.","metadata":{}},{"cell_type":"code","source":"for key, value in Product_Aspect_Contraint_dict.items():\n  print(key)\n  print(value)\n  break\n\n\nfor key, value in QuestionDict.items():\n  print(key)\n  print(value)\n  break","metadata":{"id":"gljuqCZV3VxZ","outputId":"a6484b9c-a851-44b8-c0cf-581ae5b79c36","execution":{"iopub.status.busy":"2024-03-29T08:32:26.444262Z","iopub.execute_input":"2024-03-29T08:32:26.444590Z","iopub.status.idle":"2024-03-29T08:32:26.451585Z","shell.execute_reply.started":"2024-03-29T08:32:26.444566Z","shell.execute_reply":"2024-03-29T08:32:26.450605Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"1\n[['O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P B-P O-P'], ['O-A O-A O-A O-A O-A O-A O-A B-A I-A I-A I-A O-A O-A O-A B-A I-A O-A B-A O-A O-A O-A O-A O-A'], ['O-C O-C O-C O-C B-C B-C I-C I-C I-C I-C I-C O-C O-C O-C O-C O-C O-C O-C O-C B-C I-C I-C O-C']]\n1\nWhat are the best smartphones with a built in stylus feature with a good quality display and RAM , other than Samsung ?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Constructing the Data Structure for Model Input\n\nAfter our initial exploration, the next step involves structuring our data into a format suitable for our Element Extraction model. The provided code snippet accomplishes this by iterating over the `QuestionDict` and corresponding `Product_Aspect_Contraint_dict` to create a comprehensive list of dictionaries, each containing a question and its associated entity, aspect, and constraint labels. This structured approach facilitates the easy manipulation and analysis of our dataset, preparing it for the model training phase. By aligning our questions with their respective labels in a unified data structure, we ensure a smooth transition into the model's training and evaluation stages.","metadata":{}},{"cell_type":"code","source":"data = []\n\nfor key, value in QuestionDict.items():\n    question_text = value  # The question text from QuestionDict\n    label_info = Product_Aspect_Contraint_dict[key]  # The corresponding labels from Product_Aspect_Constraint_dict\n\n    # Extract label lists directly from label_info without trying to split them\n    entity_labels, aspect_labels, constraint_labels = label_info\n\n    # Since label_info items are already lists, we can directly use them\n    # Adjust the structure as needed based on your exact format\n\n    data_entry = {\n        \"text\": question_text,\n        \"entity_labels\": entity_labels,  # No need to wrap in another list or call split\n        \"aspect_labels\": aspect_labels,\n        \"constraint_labels\": constraint_labels\n    }\n\n    data.append(data_entry)\n","metadata":{"id":"e0EkdfA93VxZ","execution":{"iopub.status.busy":"2024-03-29T08:32:26.452642Z","iopub.execute_input":"2024-03-29T08:32:26.452895Z","iopub.status.idle":"2024-03-29T08:32:26.464770Z","shell.execute_reply.started":"2024-03-29T08:32:26.452874Z","shell.execute_reply":"2024-03-29T08:32:26.463823Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Previewing Structured Data Entries\n\nTo verify the integrity and structure of our newly constructed data entries, we print the first item from our prepared data list. This step is crucial for ensuring that our data is correctly formatted and contains all necessary information for the Element Extraction tasks. By examining this sample entry, we can confirm the successful preparation of our data, setting the stage for the subsequent model training and evaluation processes.","metadata":{}},{"cell_type":"code","source":"for item in data:\n    print(item)\n    #print(item.type)\n    break","metadata":{"id":"3K5gP_dC3VxZ","outputId":"f9601058-0a0d-4db5-d32f-724c7c4969f2","execution":{"iopub.status.busy":"2024-03-29T08:32:26.465871Z","iopub.execute_input":"2024-03-29T08:32:26.466681Z","iopub.status.idle":"2024-03-29T08:32:26.474565Z","shell.execute_reply.started":"2024-03-29T08:32:26.466651Z","shell.execute_reply":"2024-03-29T08:32:26.473715Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{'text': 'What are the best smartphones with a built in stylus feature with a good quality display and RAM , other than Samsung ?', 'entity_labels': ['O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P O-P B-P O-P'], 'aspect_labels': ['O-A O-A O-A O-A O-A O-A O-A B-A I-A I-A I-A O-A O-A O-A B-A I-A O-A B-A O-A O-A O-A O-A O-A'], 'constraint_labels': ['O-C O-C O-C O-C B-C B-C I-C I-C I-C I-C I-C O-C O-C O-C O-C O-C O-C O-C O-C B-C I-C I-C O-C']}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Refining Label Formats for Analysis\n\nBefore we proceed with training our Element Extraction model, it's imperative to refine the format of our labels to ensure compatibility with our processing algorithms. This code snippet updates each data entry by splitting the label strings into lists of individual labels. This step transforms our previously structured label information into a more granular and model-friendly format, enabling precise tagging and classification during the training phase. By adjusting the label format, we enhance the model's ability to learn from and accurately predict the elements within our dataset.","metadata":{}},{"cell_type":"code","source":"# Iterate through each dictionary in the list\nfor item in data:\n    # Splitting the labels based on space ' ' and updating the values\n    item['entity_labels'] = item['entity_labels'][0].split()\n    item['aspect_labels'] = item['aspect_labels'][0].split()\n    item['constraint_labels'] = item['constraint_labels'][0].split()\n\n#print(data)\nfor item in data:\n    print(item)\n    break","metadata":{"id":"P1nclnUa3VxZ","outputId":"f31e6c02-ed40-4638-df2d-1d5599651465","execution":{"iopub.status.busy":"2024-03-29T08:32:26.475657Z","iopub.execute_input":"2024-03-29T08:32:26.475906Z","iopub.status.idle":"2024-03-29T08:32:26.489932Z","shell.execute_reply.started":"2024-03-29T08:32:26.475885Z","shell.execute_reply":"2024-03-29T08:32:26.489082Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'text': 'What are the best smartphones with a built in stylus feature with a good quality display and RAM , other than Samsung ?', 'entity_labels': ['O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'O-P', 'B-P', 'O-P'], 'aspect_labels': ['O-A', 'O-A', 'O-A', 'O-A', 'O-A', 'O-A', 'O-A', 'B-A', 'I-A', 'I-A', 'I-A', 'O-A', 'O-A', 'O-A', 'B-A', 'I-A', 'O-A', 'B-A', 'O-A', 'O-A', 'O-A', 'O-A', 'O-A'], 'constraint_labels': ['O-C', 'O-C', 'O-C', 'O-C', 'B-C', 'B-C', 'I-C', 'I-C', 'I-C', 'I-C', 'I-C', 'O-C', 'O-C', 'O-C', 'O-C', 'O-C', 'O-C', 'O-C', 'O-C', 'B-C', 'I-C', 'I-C', 'O-C']}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Applying Label Encoding for Machine Learning\n\nThe final preprocessing step involves converting our textual labels into numerical representations, a process known as label encoding. This transformation is crucial for machine learning models, which require numerical input. Using a predefined mapping dictionary, we replace the textual labels in our dataset with their corresponding numerical codes. This encoding not only simplifies the model's input but also ensures that the training process is optimized for efficiency and accuracy. By examining a sample entry after this transformation, we can confirm the readiness of our data for the upcoming machine learning tasks.","metadata":{}},{"cell_type":"code","source":"# Mapping dictionary\nlabel_map = {\"O-P\": 0, \"B-P\": 1, \"I-P\": 2, \"O-A\": 0, \"B-A\": 1, \"I-A\": 2, \"O-C\": 0, \"B-C\": 1, \"I-C\": 2}\n\n# Iterate through each dictionary in the list\nfor item in data:\n    # Replace entity_labels\n    item['entity_labels'] = [label_map[label] for label in item['entity_labels']]\n    # Replace aspect_labels\n    item['aspect_labels'] = [label_map[label] for label in item['aspect_labels']]\n    # Replace constraint_labels\n    item['constraint_labels'] = [label_map[label] for label in item['constraint_labels']]\n\n    #print(data)\nfor item in data:\n    print(item)\n    break","metadata":{"id":"U86qC0233VxZ","outputId":"d465891f-646b-4f78-8642-61eb88a234c6","execution":{"iopub.status.busy":"2024-03-29T08:32:26.490884Z","iopub.execute_input":"2024-03-29T08:32:26.491127Z","iopub.status.idle":"2024-03-29T08:32:26.507153Z","shell.execute_reply.started":"2024-03-29T08:32:26.491106Z","shell.execute_reply":"2024-03-29T08:32:26.506269Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"{'text': 'What are the best smartphones with a built in stylus feature with a good quality display and RAM , other than Samsung ?', 'entity_labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], 'aspect_labels': [0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0], 'constraint_labels': [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Displaying Processed Data and Labels\n\nTo confirm that our data is correctly processed and ready for the next steps, we display the text, entity labels, aspect labels, and constraint labels of the first entry in our prepared dataset. This visualization allows us to ensure that the textual data aligns with its corresponding numerical labels, indicating successful label encoding and data structuring. It's a vital checkpoint to verify the data's readiness for model training and analysis.","metadata":{}},{"cell_type":"code","source":"print(data[0]['text'])\nprint(data[0]['entity_labels'])\nprint(data[0]['aspect_labels'])\nprint(data[0]['constraint_labels'])","metadata":{"id":"nwNvX63u3Vxo","outputId":"234e408b-e611-4a56-b5f0-bf55fd1fdf11","execution":{"iopub.status.busy":"2024-03-29T08:32:26.508296Z","iopub.execute_input":"2024-03-29T08:32:26.508847Z","iopub.status.idle":"2024-03-29T08:32:26.517458Z","shell.execute_reply.started":"2024-03-29T08:32:26.508819Z","shell.execute_reply":"2024-03-29T08:32:26.516502Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"What are the best smartphones with a built in stylus feature with a good quality display and RAM , other than Samsung ?\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n[0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0]\n[0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Organizing Data for Easy Access\n\nAs we move forward, organizing our data in a manner that facilitates easy access and manipulation becomes crucial. By categorizing the processed data into separate dictionaries for entities, aspects, and constraints, we enhance our ability to swiftly retrieve and analyze specific segments of our dataset. This organization not only aids in data management but also in performing focused analyses on different elements of the text. Displaying the structured data for the first entry further solidifies our understanding of the dataset's organization and prepares us for detailed element extraction tasks.","metadata":{}},{"cell_type":"code","source":"# Assuming \"data\" is a list of dictionaries, each structured like the example you provided\n\n# Initialize three empty dictionaries\nentity_dict, aspect_dict, constraint_dict = {}, {}, {}\n\n# Iterate through each item in the data list\nfor item in data:\n    # Extract and assign the relevant information to each dictionary\n    entity_dict[item['text']] = item['entity_labels']\n    aspect_dict[item['text']] = item['aspect_labels']\n    constraint_dict[item['text']] = item['constraint_labels']\n\n# Example to show the result for the first item in the list\nprint(\"Text:\", data[0]['text'])\nprint(\"Entity Labels:\", entity_dict[data[0]['text']])\nprint(\"Aspect Labels:\", aspect_dict[data[0]['text']])\nprint(\"Constraint Labels:\", constraint_dict[data[0]['text']])\n","metadata":{"id":"SXwpN31c3Vxo","outputId":"bd90871b-acca-4a14-a674-e8949ca30d8f","execution":{"iopub.status.busy":"2024-03-29T08:32:26.521948Z","iopub.execute_input":"2024-03-29T08:32:26.522811Z","iopub.status.idle":"2024-03-29T08:32:26.531172Z","shell.execute_reply.started":"2024-03-29T08:32:26.522781Z","shell.execute_reply":"2024-03-29T08:32:26.530376Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Text: What are the best smartphones with a built in stylus feature with a good quality display and RAM , other than Samsung ?\nEntity Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\nAspect Labels: [0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0]\nConstraint Labels: [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Quick Access to Data Entries\n\nTo demonstrate the utility of our organized data structure, we print the first key-value pair from the `entity_dict`. This example showcases how our organized data facilitates quick access to specific information, in this case, entity labels associated with a particular text. It exemplifies the efficient retrieval of data for analysis or further processing, an essential feature for handling complex NLP tasks like Element Extraction.","metadata":{}},{"cell_type":"code","source":"for key, value in entity_dict.items():\n    print(key)\n    print(value)\n    break","metadata":{"id":"08Odapdn3Vxo","outputId":"9785537c-55bc-4dfe-f704-70e7de432c26","execution":{"iopub.status.busy":"2024-03-29T08:32:26.532169Z","iopub.execute_input":"2024-03-29T08:32:26.532449Z","iopub.status.idle":"2024-03-29T08:32:26.540280Z","shell.execute_reply.started":"2024-03-29T08:32:26.532428Z","shell.execute_reply":"2024-03-29T08:32:26.539292Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"What are the best smartphones with a built in stylus feature with a good quality display and RAM , other than Samsung ?\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Setting Up the Environment for Model Training\n\nBefore we can train our RoBERTa models for entity, aspect, and constraint classification, it is essential to set up our Python environment with the necessary libraries. This involves installing the `transformers` library, which provides the RoBERTa model and tokenizer, as well as `torch` for working with tensors, and `sklearn` for model evaluation and data manipulation utilities. By ensuring these libraries are installed, we equip our environment with the tools required for efficient model training and evaluation, paving the way for the successful application of our element extraction methodology.","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch sklearn","metadata":{"id":"mq1N4lng3Vxo","outputId":"d61fd2a0-6251-4305-c507-4672c6e20668","execution":{"iopub.status.busy":"2024-03-29T08:32:26.541360Z","iopub.execute_input":"2024-03-29T08:32:26.541656Z","iopub.status.idle":"2024-03-29T08:32:29.117699Z","shell.execute_reply.started":"2024-03-29T08:32:26.541628Z","shell.execute_reply":"2024-03-29T08:32:29.116639Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nCollecting sklearn\n  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m More information is available at\n  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\n\u001b[?25h","output_type":"stream"}]},{"cell_type":"markdown","source":"## Extracting and Demonstrating Key-Value Pair Relationships\n\nThe function `extract_values` is designed to map keys from one list to values in another, based on a reference dictionary. This is particularly useful in scenarios where we need to align different elements of our dataset, such as texts and their corresponding labels, for detailed analysis or processing. Following this, we showcase the utility of this function by preparing to align texts with their labels for entities, aspects, and constraints separately. By extracting and printing the first elements of these lists, we prepare to demonstrate how texts and labels can be effectively paired, providing a clear foundation for any subsequent operations that require a direct association between textual data and their annotated labels.","metadata":{}},{"cell_type":"code","source":"def extract_values(dict1, list1, list2):\n    result = {}\n    for key, value in zip(list1, list2):\n        result[key] = dict1.get(value, None)\n    return result\n\n\nentity_text_ = list(entity_dict.keys())\nentity_labels_ = list(entity_dict.values())\n\nprint(\"list1:\", entity_text_[0])\nprint(\"list2:\", entity_labels_[0])\n\n\naspect_text_ = list(aspect_dict.keys())\naspect_labels_ = list(aspect_dict.values())\n\nprint(\"list1:\", aspect_text_[0])\nprint(\"list2:\", aspect_labels_[0])\n\n\nconstraint_text_ = list(constraint_dict.keys())\nconstraint_labels_ = list(constraint_dict.values())\n\nprint(\"list1:\", constraint_text_[0])\nprint(\"list2:\", constraint_labels_[0])\n","metadata":{"id":"Z-WdI96R3Vxo","outputId":"fc2996f7-24f5-48e6-d518-68f3aba2c57a","execution":{"iopub.status.busy":"2024-03-29T08:32:29.119141Z","iopub.execute_input":"2024-03-29T08:32:29.119457Z","iopub.status.idle":"2024-03-29T08:32:29.130220Z","shell.execute_reply.started":"2024-03-29T08:32:29.119429Z","shell.execute_reply":"2024-03-29T08:32:29.129243Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"list1: What are the best smartphones with a built in stylus feature with a good quality display and RAM , other than Samsung ?\nlist2: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\nlist1: What are the best smartphones with a built in stylus feature with a good quality display and RAM , other than Samsung ?\nlist2: [0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0]\nlist1: What are the best smartphones with a built in stylus feature with a good quality display and RAM , other than Samsung ?\nlist2: [0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preparing Data for RoBERTa Token Classification\n\nTo fine-tune the RoBERTa model for token classification, we first import necessary modules and initialize the RoBERTa tokenizer and model with the appropriate number of labels for our classification task. We then proceed to preprocess our entity texts and labels for model input. This involves tokenizing the texts, creating attention masks for handling padding, and adjusting label lengths to match the tokenized inputs. Padding labels with -100 ensures that these positions are ignored during the loss calculation, aligning with the model's requirements. By converting these lists into tensors, we prepare our dataset for training with RoBERTa, setting the stage for effective learning and classification of entities, aspects, and constraints within our text data.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaTokenizer, RobertaForTokenClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=3)\n\n\nmax_length = max1  # Adjust as needed\n\ninput_ids = []\nattention_masks = []\nentity_labels = []\n\ntexts=[]\nlabels=[]\n\ntexts = entity_text_\nlabels = entity_labels_\n\nfor text, label in zip(texts, labels):\n    encoded_dict = tokenizer.encode_plus(\n                        text,\n                        add_special_tokens=True,\n                        max_length=max_length,\n                        padding='max_length',\n                        truncation=True,\n                        return_attention_mask=True,\n                        return_tensors='pt',\n                    )\n\n    # Create a mask for the labels to ignore padding in the loss computation\n    label_mask = [1] * len(label) + [0] * (max_length - len(label))\n    label = label + [-100] * (max_length - len(label))  # Padding labels with -100, which is ignored by the loss function\n\n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n    entity_labels.append(label)\n\nentity_input_ids = torch.cat(input_ids, dim=0)\nentity_attention_masks = torch.cat(attention_masks, dim=0)\nentity_labels = torch.tensor(entity_labels)\n\n# Proceed with dataset creation, DataLoader setup, and cross-validation as before\n","metadata":{"id":"oSeyfyRX3Vxo","outputId":"084ac291-026e-42f7-a19d-e9d7a894b275","execution":{"iopub.status.busy":"2024-03-29T08:32:29.131456Z","iopub.execute_input":"2024-03-29T08:32:29.131726Z","iopub.status.idle":"2024-03-29T08:32:43.981846Z","shell.execute_reply.started":"2024-03-29T08:32:29.131697Z","shell.execute_reply":"2024-03-29T08:32:43.981071Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca8fd4a94694bd193d14d6cdde27bbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34327f16097c433a8c9c387e6e24d478"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c7dbcdb22d94262864d55e724ff6ed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4066b892fedf4c2684ee46ca1c3077ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63cecaa6ed2f41c68077040f34d680a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98de13019ac64da3afa7fc08d8d71053"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preparing Aspect Data for RoBERTa Token Classification\n\nFor the aspect classification task, we proceed to tokenize the aspect-related texts using the RoBERTa tokenizer, similarly to how we handled entity data. This process involves converting texts into input IDs, generating attention masks to handle different sequence lengths, and padding the labels to match the tokenized input lengths. Special care is taken to pad labels with -100, a value indicating positions that should be ignored during the model's loss calculation, ensuring accurate training and classification. The resulting tensors for input IDs, attention masks, and aspect labels are then ready to be utilized in creating a dataset specifically designed for aspect classification with RoBERTa, setting a clear path towards identifying and categorizing aspect-related elements within texts.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaTokenizer, RobertaForTokenClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=3)\n\n\nmax_length = max1  # Adjust as needed\n\ninput_ids = []\nattention_masks = []\naspect_labels = []\n\ntexts=[]\nlabels=[]\n\ntexts = aspect_text_\nlabels = aspect_labels_\n\nfor text, label in zip(texts, labels):\n    encoded_dict = tokenizer.encode_plus(\n                        text,\n                        add_special_tokens=True,\n                        max_length=max_length,\n                        padding='max_length',\n                        truncation=True,\n                        return_attention_mask=True,\n                        return_tensors='pt',\n                    )\n\n    # Create a mask for the labels to ignore padding in the loss computation\n    label_mask = [1] * len(label) + [0] * (max_length - len(label))\n    label = label + [-100] * (max_length - len(label))  # Padding labels with -100, which is ignored by the loss function\n\n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n    aspect_labels.append(label)\n\naspect_input_ids = torch.cat(input_ids, dim=0)\naspect_attention_masks = torch.cat(attention_masks, dim=0)\naspect_labels = torch.tensor(aspect_labels)\n\n# Proceed with dataset creation, DataLoader setup, and cross-validation as before\n","metadata":{"id":"DbdufHe_3Vxo","outputId":"5e160885-88d4-40d3-c8d4-d79ad031379f","execution":{"iopub.status.busy":"2024-03-29T08:32:43.982901Z","iopub.execute_input":"2024-03-29T08:32:43.983346Z","iopub.status.idle":"2024-03-29T08:32:45.470550Z","shell.execute_reply.started":"2024-03-29T08:32:43.983309Z","shell.execute_reply":"2024-03-29T08:32:45.469759Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preparing Constraint Data for RoBERTa Token Classification\n\nIn parallel with entities and aspects, the preparation of constraint-related data follows a structured approach to ensure the RoBERTa model can effectively learn and classify constraint information. Through the process of tokenizing constraint texts, we generate input IDs and attention masks, alongside adjusting label lengths through padding to align with the tokenized inputs. Padding the labels with -100 is crucial for correctly informing the model which positions to disregard in its loss calculations. This meticulous preparation results in tensors for constraint input IDs, attention masks, and labels, all set for integration into a dataset tailored for the constraint classification task. By systematically preparing our data for each specific element type, we enable the RoBERTa model to finely distinguish and accurately extract constraint elements from our texts, enhancing the overall capability of our element extraction system.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaTokenizer, RobertaForTokenClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=3)\n\n\nmax_length = max1  # Adjust as needed\n\ninput_ids = []\nattention_masks = []\nconstraint_labels = []\n\ntexts=[]\nlabels=[]\n\ntexts = constraint_text_\nlabels = constraint_labels_\n\nfor text, label in zip(texts, labels):\n    encoded_dict = tokenizer.encode_plus(\n                        text,\n                        add_special_tokens=True,\n                        max_length=max_length,\n                        padding='max_length',\n                        truncation=True,\n                        return_attention_mask=True,\n                        return_tensors='pt',\n                    )\n\n    # Create a mask for the labels to ignore padding in the loss computation\n    label_mask = [1] * len(label) + [0] * (max_length - len(label))\n    label = label + [-100] * (max_length - len(label))  # Padding labels with -100, which is ignored by the loss function\n\n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n    constraint_labels.append(label)\n\nconstraint_input_ids = torch.cat(input_ids, dim=0)\nconstraint_attention_masks = torch.cat(attention_masks, dim=0)\nconstraint_labels = torch.tensor(constraint_labels)\n\n# Proceed with dataset creation, DataLoader setup, and cross-validation as before\n","metadata":{"id":"fZ_M6Y0f3Vxo","outputId":"ee1398b5-c2d6-415e-c758-5c7227eda553","execution":{"iopub.status.busy":"2024-03-29T08:32:45.471637Z","iopub.execute_input":"2024-03-29T08:32:45.471922Z","iopub.status.idle":"2024-03-29T08:32:46.656274Z","shell.execute_reply.started":"2024-03-29T08:32:45.471898Z","shell.execute_reply":"2024-03-29T08:32:46.654811Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Inspecting the Prepared Datasets for RoBERTa Classification\n\nAfter preprocessing our data for entity, aspect, and constraint classification, we examine the first entries of our prepared datasets to ensure their readiness for model training. This includes reviewing the input IDs and attention masks for each classification type, which are essential for informing the RoBERTa model about the text to focus on and the padding to ignore. Additionally, we inspect the labels for entities, aspects, and constraints, confirming that they have been correctly padded and are in the appropriate format for the classification tasks. This verification step is crucial for ensuring the integrity and correctness of our datasets before proceeding with the model training process, setting a solid foundation for successful element extraction.","metadata":{}},{"cell_type":"code","source":"print(entity_input_ids[0])\nprint(entity_attention_masks[0])\nprint(aspect_input_ids[0])\nprint(aspect_attention_masks[0])\nprint(constraint_input_ids[0])\nprint(constraint_attention_masks[0])\nprint(entity_labels[0])\nprint(aspect_labels[0])\nprint(constraint_labels[0])","metadata":{"id":"B7Y1eQ6M3Vxo","outputId":"1f2520fa-5525-4033-d970-580d157fc785","execution":{"iopub.status.busy":"2024-03-29T08:32:46.658082Z","iopub.execute_input":"2024-03-29T08:32:46.658448Z","iopub.status.idle":"2024-03-29T08:32:46.675373Z","shell.execute_reply.started":"2024-03-29T08:32:46.658418Z","shell.execute_reply":"2024-03-29T08:32:46.674492Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"tensor([    0,  2264,    32,     5,   275,  7466,    19,    10,  1490,    11,\n        15240,   687,  1905,    19,    10,   205,  1318,  2332,     8, 10646,\n         2156,    97,    87,  3797, 17487,     2,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0])\ntensor([    0,  2264,    32,     5,   275,  7466,    19,    10,  1490,    11,\n        15240,   687,  1905,    19,    10,   205,  1318,  2332,     8, 10646,\n         2156,    97,    87,  3797, 17487,     2,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0])\ntensor([    0,  2264,    32,     5,   275,  7466,    19,    10,  1490,    11,\n        15240,   687,  1905,    19,    10,   205,  1318,  2332,     8, 10646,\n         2156,    97,    87,  3797, 17487,     2,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0])\ntensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100])\ntensor([   0,    0,    0,    0,    0,    0,    0,    1,    2,    2,    2,    0,\n           0,    0,    1,    2,    0,    1,    0,    0,    0,    0,    0, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100])\ntensor([   0,    0,    0,    0,    1,    1,    2,    2,    2,    2,    2,    0,\n           0,    0,    0,    0,    0,    0,    0,    1,    2,    2,    0, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Setting Up the Environment\n\nBefore diving into the model training and evaluation, it's essential to ensure that our environment is equipped with the necessary libraries and frameworks. The installation commands provided here are for installing PyTorch (`torch`, `torchvision`, `torchaudio`) along with `transformers` and `scikit-learn`. PyTorch serves as the backbone for building and training neural network models, `transformers` provides access to pre-trained models like RoBERTa and utilities for NLP tasks, and `scikit-learn` offers tools for data processing and evaluation metrics. Executing these commands prepares the environment with the required dependencies, setting the stage for the development and evaluation of our Multi-Task Learning model for Element Extraction.","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision torchaudio\n!pip install transformers\n!pip install scikit-learn","metadata":{"id":"4T0S7LNw97ah","outputId":"d4a55e59-ba75-4a9b-ec88-0e85a1ec8823","execution":{"iopub.status.busy":"2024-03-29T08:32:46.676265Z","iopub.execute_input":"2024-03-29T08:32:46.676532Z","iopub.status.idle":"2024-03-29T08:33:23.584950Z","shell.execute_reply.started":"2024-03-29T08:32:46.676510Z","shell.execute_reply":"2024-03-29T08:33:23.583659Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preparing Inputs for Model Training\n\nIn this step, we prepare the `input_ids` and `attention_masks` for the model training process. Notably, while we have separate variables for entities, aspects, and constraints (`entity_input_ids`, `aspect_input_ids`, `constraint_input_ids` and their corresponding attention masks), the data preparation process ensures that these variables are identical in structure and content. This uniformity allows","metadata":{}},{"cell_type":"code","source":"input_ids = constraint_input_ids\nattention_masks = constraint_attention_masks","metadata":{"id":"X_XGhCOqALQQ","execution":{"iopub.status.busy":"2024-03-29T08:33:23.586602Z","iopub.execute_input":"2024-03-29T08:33:23.586917Z","iopub.status.idle":"2024-03-29T08:33:23.597220Z","shell.execute_reply.started":"2024-03-29T08:33:23.586892Z","shell.execute_reply":"2024-03-29T08:33:23.596294Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Implementing a Multi-Task Learning Framework with Adapter for Element Extraction\n\nThis code segment introduces a Multi-Task Learning (MTL) framework specifically designed for Element Extraction, integrating classifications for Entity, Aspect, and Constraint within a unified model architecture. The `MultiTaskRoberta` class extends PyTorch's `Module`, incorporating a RoBERTa-based transformer model enhanced with the \"AdapterHub/roberta-base-pf-qnli\" adapter. This adapter is meticulously engineered to improve performance on question-answering tasks by fine-tuning the model to adeptly handle the intricacies of natural language inference.\n\nThe adapter's integration allows the model to leverage deeper contextual insights from the pre-trained RoBERTa model, specifically tuned to meet the unique requirements of the Element Extraction tasks. By incorporating the adapter, the model gains enhanced capabilities to detect subtle nuances and interrelationships within the data, which are crucial for precise classification of entities, aspects, and constraints.\n\nFollowing this model setup, the `TokenClassificationDataset` class efficiently organizes token sequences and their corresponding labels, ensuring the data is optimally structured for effective training. The subsequent application of K-Fold cross-validation not only tests the robustness of the model across different data subsets but also bolsters its generalization capabilities. This iterative training and evaluation process across each fold offers detailed insights into the model's performance, showcasing the significant benefits of integrating the adapter within the MTL framework to tackle complex NLP challenges such as Element Extraction.\n","metadata":{}},{"cell_type":"code","source":"!pip install adapters","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:33:23.598260Z","iopub.execute_input":"2024-03-29T08:33:23.598602Z","iopub.status.idle":"2024-03-29T08:33:46.226442Z","shell.execute_reply.started":"2024-03-29T08:33:23.598568Z","shell.execute_reply":"2024-03-29T08:33:46.225251Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Collecting adapters\n  Downloading adapters-0.1.2-py3-none-any.whl.metadata (15 kB)\nCollecting transformers~=4.36.0 (from adapters)\n  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers~=4.36.0->adapters) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers~=4.36.0->adapters) (0.21.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers~=4.36.0->adapters) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers~=4.36.0->adapters) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers~=4.36.0->adapters) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers~=4.36.0->adapters) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers~=4.36.0->adapters) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers~=4.36.0->adapters) (0.15.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers~=4.36.0->adapters) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers~=4.36.0->adapters) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers~=4.36.0->adapters) (2024.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers~=4.36.0->adapters) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers~=4.36.0->adapters) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers~=4.36.0->adapters) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers~=4.36.0->adapters) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers~=4.36.0->adapters) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers~=4.36.0->adapters) (2024.2.2)\nDownloading adapters-0.1.2-py3-none-any.whl (256 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/256.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers, adapters\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.2\n    Uninstalling transformers-4.38.2:\n      Successfully uninstalled transformers-4.38.2\nSuccessfully installed adapters-0.1.2 transformers-4.36.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, Subset, RandomSampler\nfrom transformers import RobertaModel\nfrom torch.optim import AdamW,Adam\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom transformers import RobertaModel, RobertaConfig\nimport adapters \n","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:33:46.234424Z","iopub.execute_input":"2024-03-29T08:33:46.234703Z","iopub.status.idle":"2024-03-29T08:33:46.248606Z","shell.execute_reply.started":"2024-03-29T08:33:46.234679Z","shell.execute_reply":"2024-03-29T08:33:46.247818Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, Subset, RandomSampler\nfrom transformers import RobertaModel\nfrom torch.optim import AdamW,Adam\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom transformers import RobertaModel, RobertaConfig\n\n\n# Define the model\nclass MultiTaskRoberta(torch.nn.Module):\n    def __init__(self, num_labels_entity, num_labels_aspect, num_labels_constraint):\n        super().__init__()\n        self.roberta = RobertaModel.from_pretrained('roberta-base')\n                  \n        adapters.init(self.roberta)\n        adapter_name = self.roberta.load_adapter(\"AdapterHub/roberta-base-pf-qnli\", source=\"hf\")\n        self.roberta.set_active_adapters(adapter_name)\n        \n        self.classifier_entity = torch.nn.Linear(self.roberta.config.hidden_size, num_labels_entity)\n        self.classifier_aspect = torch.nn.Linear(self.roberta.config.hidden_size, num_labels_aspect)\n        self.classifier_constraint = torch.nn.Linear(self.roberta.config.hidden_size, num_labels_constraint)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_output = outputs.last_hidden_state\n        logits_entity = self.classifier_entity(sequence_output)\n        logits_aspect = self.classifier_aspect(sequence_output)\n        logits_constraint = self.classifier_constraint(sequence_output)\n        return logits_entity, logits_aspect, logits_constraint\n\n# Define your dataset class\nclass TokenClassificationDataset(Dataset):\n    def __init__(self, input_ids, attention_masks, entity_labels, aspect_labels, constraint_labels):\n        self.input_ids = input_ids\n        self.attention_masks = attention_masks\n        self.entity_labels = entity_labels\n        self.aspect_labels = aspect_labels\n        self.constraint_labels = constraint_labels\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.input_ids[idx].clone().detach(),\n            'attention_mask': self.attention_masks[idx].clone().detach(),\n            'labels_entity': self.entity_labels[idx].clone().detach(),\n            'labels_aspect': self.aspect_labels[idx].clone().detach(),\n            'labels_constraint': self.constraint_labels[idx].clone().detach(),\n        }\n\n\n\nnum_samples = 1257\nseq_length = 100\nvocab_size = 30522\nnum_labels_entity = 3\nnum_labels_aspect = 3\nnum_labels_constraint = 3\n\n\nlabels_entity = entity_labels\nlabels_aspect = aspect_labels\nlabels_constraint = constraint_labels\n\n# Instantiate the dataset\ndataset = TokenClassificationDataset(input_ids, attention_masks, labels_entity, labels_aspect, labels_constraint)\n\n# Training and evaluation setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = MultiTaskRoberta(num_labels_entity, num_labels_aspect, num_labels_constraint).to(device)\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=-100)  # Use -100 for padding token labels\n\n# Prepare for K-Fold cross-validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nfold_results = {}\n\nfor fold, (train_ids, test_ids) in enumerate(kf.split(dataset)):\n    # DataLoaders for the current fold\n    train_dataset = Subset(dataset, train_ids)\n    test_dataset = Subset(dataset, test_ids)\n    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n    test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)  # No shuffling for test data\n\n    # Reinitialize the model and optimizer at the start of each fold\n    model = MultiTaskRoberta(num_labels_entity, num_labels_aspect, num_labels_constraint).to(device)\n    #optimizer = Adam(model.parameters(), lr=3e-5)\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n\n\n    # Training loop\n    for epoch in range(5):  # Number of epochs\n        model.train()\n        for batch in tqdm(train_dataloader, desc=f\"Training Fold {fold+1} - Epoch {epoch+1}\"):\n          input_ids = batch['input_ids'].to(device)\n          attention_mask = batch['attention_mask'].to(device)\n          labels_entity = batch['labels_entity'].to(device)\n          labels_aspect = batch['labels_aspect'].to(device)\n          labels_constraint = batch['labels_constraint'].to(device)\n\n          optimizer.zero_grad()\n          logits_entity, logits_aspect, logits_constraint = model(input_ids, attention_mask)\n\n          # Active loss mask for ignoring -100 labels for entity\n          active_loss_entity = labels_entity.view(-1) != -100\n          active_logits_entity = logits_entity.view(-1, num_labels_entity)[active_loss_entity]\n          active_labels_entity = labels_entity.view(-1)[active_loss_entity]\n          loss_entity = criterion(active_logits_entity, active_labels_entity)\n\n          # Active loss mask for ignoring -100 labels for aspect\n          active_loss_aspect = labels_aspect.view(-1) != -100\n          active_logits_aspect = logits_aspect.view(-1, num_labels_aspect)[active_loss_aspect]\n          active_labels_aspect = labels_aspect.view(-1)[active_loss_aspect]\n          loss_aspect = criterion(active_logits_aspect, active_labels_aspect)\n\n          # Active loss mask for ignoring -100 labels for constraint\n          active_loss_constraint = labels_constraint.view(-1) != -100\n          active_logits_constraint = logits_constraint.view(-1, num_labels_constraint)[active_loss_constraint]\n          active_labels_constraint = labels_constraint.view(-1)[active_loss_constraint]\n          loss_constraint = criterion(active_logits_constraint, active_labels_constraint)\n\n          # Aggregate losses and perform a backward pass\n          total_loss = loss_entity + loss_aspect + loss_constraint\n          total_loss.backward()\n          optimizer.step()\n\n\n     \n\n\n\n    # Evaluation loop\n    model.eval()\n    with torch.no_grad():\n        all_labels_entity, all_preds_entity = [], []\n        all_labels_aspect, all_preds_aspect = [], []\n        all_labels_constraint, all_preds_constraint = [], []\n\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels_entity = batch['labels_entity'].to(device)\n            labels_aspect = batch['labels_aspect'].to(device)\n            labels_constraint = batch['labels_constraint'].to(device)\n\n            logits_entity, logits_aspect, logits_constraint = model(input_ids, attention_mask)\n\n            preds_entity = torch.argmax(logits_entity, dim=2).detach().cpu().numpy()\n            preds_aspect = torch.argmax(logits_aspect, dim=2).detach().cpu().numpy()\n            preds_constraint = torch.argmax(logits_constraint, dim=2).detach().cpu().numpy()\n\n            labels_entity = labels_entity.cpu().numpy()\n            labels_aspect = labels_aspect.cpu().numpy()\n            labels_constraint = labels_constraint.cpu().numpy()\n\n            # Filter out '-100' used for padding in labels\n            active_labels_entity = labels_entity != -100\n            active_labels_aspect = labels_aspect != -100\n            active_labels_constraint = labels_constraint != -100\n\n            preds_entity = preds_entity[active_labels_entity]\n            preds_aspect = preds_aspect[active_labels_aspect]\n            preds_constraint = preds_constraint[active_labels_constraint]\n\n            labels_entity = labels_entity[active_labels_entity]\n            labels_aspect = labels_aspect[active_labels_aspect]\n            labels_constraint = labels_constraint[active_labels_constraint]\n\n            precision_entity, recall_entity, f1_entity, _ = precision_recall_fscore_support(labels_entity, preds_entity, average='macro', zero_division=0)\n            precision_aspect, recall_aspect, f1_aspect, _ = precision_recall_fscore_support(labels_aspect, preds_aspect, average='macro', zero_division=0)\n            precision_constraint, recall_constraint, f1_constraint, _ = precision_recall_fscore_support(labels_constraint, preds_constraint, average='macro', zero_division=0)\n\n            fold_results[fold+1] = {\n                'Entity': {'Precision': precision_entity, 'Recall': recall_entity, 'F1': f1_entity},\n                'Aspect': {'Precision': precision_aspect, 'Recall': recall_aspect, 'F1': f1_aspect},\n                'Constraint': {'Precision': precision_constraint, 'Recall': recall_constraint, 'F1': f1_constraint}\n            }\n\n# Print out fold results\nfor fold, metrics in fold_results.items():\n    print(f\"\\nFold {fold} Results:\")\n    for task, scores in metrics.items():\n        print(f\"{task} - Precision: {scores['Precision']:.4f}, Recall: {scores['Recall']:.4f}, F1-Score: {scores['F1']:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:33:46.249922Z","iopub.execute_input":"2024-03-29T08:33:46.250194Z","iopub.status.idle":"2024-03-29T08:43:05.702008Z","shell.execute_reply.started":"2024-03-29T08:33:46.250163Z","shell.execute_reply":"2024-03-29T08:43:05.700929Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b68c8455d9847a5aacef1bb7ccce336"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model_head.bin:   0%|          | 0.00/2.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e3fea1d4dcd4abeb2a455fa00a33a32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dbc102514294bd7a5bcd50aaffd0219"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/575 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bd707886a034b6fad0303b76a9e4575"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4695da9ac4b94d0bbd55841f19a5ccc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_adapter.bin:   0%|          | 0.00/3.59M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2513ca233a143d889e65f11c72bdd98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"head_config.json:   0%|          | 0.00/391 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c99de0fd3eb94c24826169d5e2386067"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcfe9163e57343aeb08c51a6dced0742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 1 - Epoch 1:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"840f55d68451489e8095afefb9ed9969"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 1 - Epoch 2:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f37717a0a197461e95540f7b53bd5a4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 1 - Epoch 3:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d6bffb176004847b67288c4ed0482b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 1 - Epoch 4:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a890d1fe71f04e109c2c9c606b7e66e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 1 - Epoch 5:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6af4552efe8456c8e2eadf06c3beb90"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7a062dfcbce4aebb93df527ef0b03d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 2 - Epoch 1:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dbc34dc6c3743fabe79c8c10fd00cc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 2 - Epoch 2:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fca94ac39ef94eaeaaf91a3902cfee05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 2 - Epoch 3:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a70e12030dc48678952ce9c074c3d52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 2 - Epoch 4:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c53c00123b8a4768bdcb8d1f0f895663"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 2 - Epoch 5:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b99f68e9065e4ffda21d922c1331e0a9"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"838be7a6b2cc4463ae9088846497e962"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 3 - Epoch 1:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"374430fc838a40549183f03921020792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 3 - Epoch 2:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c98fca1519b4177bbb195bd848b75ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 3 - Epoch 3:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def6bfc1172f48de914a7d43aa9f66f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 3 - Epoch 4:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3af782c0a7a1458c8ef53469090d4958"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 3 - Epoch 5:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c73ba34e51c492d94989a48283e2ac2"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0ff38a961de4af5986cd07986cb1a1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 4 - Epoch 1:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"064c6a4631ea4b908f092c8ff53b24fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 4 - Epoch 2:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cafb8f702ae340d5b8842ad26e4eb4a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 4 - Epoch 3:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fc137a8a815463d8085974ff33b5fed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 4 - Epoch 4:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31eea6f3184648c4949d9a15a53c0379"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 4 - Epoch 5:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32d23e378e0c48e8b5d2a6e0663aed66"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c6c42098cf84d0c862395d729047754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 5 - Epoch 1:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7cc3f499ad442d69f1a3f3bbca30380"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 5 - Epoch 2:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0661c72b81904acf84520949364d213a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 5 - Epoch 3:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6cdd5ab388a44ce9c88f6e8f3f180ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 5 - Epoch 4:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3331f09a44014197a50c5d22dc7f0f12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Fold 5 - Epoch 5:   0%|          | 0/128 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c36f744052f547b683d5deef43b0ce17"}},"metadata":{}},{"name":"stdout","text":"\nFold 1 Results:\nEntity - Precision: 0.9792, Recall: 0.9963, F1-Score: 0.9874\nAspect - Precision: 0.9524, Recall: 0.9974, F1-Score: 0.9730\nConstraint - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n\nFold 2 Results:\nEntity - Precision: 0.9815, Recall: 0.9560, F1-Score: 0.9682\nAspect - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\nConstraint - Precision: 0.5556, Recall: 0.5972, F1-Score: 0.5744\n\nFold 3 Results:\nEntity - Precision: 0.9117, Recall: 0.9047, F1-Score: 0.9081\nAspect - Precision: 0.7514, Recall: 0.7514, F1-Score: 0.7514\nConstraint - Precision: 0.6667, Recall: 0.9856, F1-Score: 0.7704\n\nFold 4 Results:\nEntity - Precision: 0.9220, Recall: 0.9172, F1-Score: 0.9185\nAspect - Precision: 0.7798, Recall: 0.9854, F1-Score: 0.8600\nConstraint - Precision: 0.9858, Recall: 0.7000, F1-Score: 0.7983\n\nFold 5 Results:\nEntity - Precision: 0.9372, Recall: 0.9674, F1-Score: 0.9511\nAspect - Precision: 0.7556, Recall: 0.9878, F1-Score: 0.8438\nConstraint - Precision: 0.8333, Recall: 0.9977, F1-Score: 0.8877\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize dictionaries to store total metrics for calculating averages\ntotal_metrics = {\n    'Entity': {'Precision': 0, 'Recall': 0, 'F1': 0},\n    'Aspect': {'Precision': 0, 'Recall': 0, 'F1': 0},\n    'Constraint': {'Precision': 0, 'Recall': 0, 'F1': 0}\n}\n\n# Sum up metrics for each task across all folds\nfor fold in fold_results.values():\n    for task, metrics in fold.items():\n        for metric, value in metrics.items():\n            total_metrics[task][metric] += value\na\n# Calculate the average for each metric for each task\naverage_metrics = {\n    task: {metric: value / len(fold_results) for metric, value in metrics.items()}\n    for task, metrics in total_metrics.items()\n}\n\nprint(average_metrics)","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:43:37.314110Z","iopub.execute_input":"2024-03-29T08:43:37.314508Z","iopub.status.idle":"2024-03-29T08:43:37.322241Z","shell.execute_reply.started":"2024-03-29T08:43:37.314479Z","shell.execute_reply":"2024-03-29T08:43:37.321276Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"{'Entity': {'Precision': 0.9463052376922583, 'Recall': 0.9483129690680914, 'F1': 0.9466488365771555}, 'Aspect': {'Precision': 0.8478252484794542, 'Recall': 0.9444035649240515, 'F1': 0.885664204020611}, 'Constraint': {'Precision': 0.8082621082621081, 'Recall': 0.8561059890262064, 'F1': 0.8061597303058671}}\n","output_type":"stream"}]}]}